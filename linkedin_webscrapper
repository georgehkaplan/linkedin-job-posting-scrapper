import requests
from bs4 import BeautifulSoup
import csv
from datetime import datetime


# URL of the page you want to scrape
BASE_URL = "https://www.linkedin.com/jobs/entry-level-financial-analyst-jobs"



# Create a CSV file and write headers to the file
FILENAME = 'finance_jobs.csv'

def generate_url(page_number, location=None, salary=None, title=None):
    url = f"{BASE_URL}?start={page_number*50}&count=400" # Change the count to the number of job postings you want to scrape
    if location:
        url += f"&location={location}" # Type in the location you want to search for
    if salary:
        url += f"&salary={salary}" # Type in the salary you want to search for
    if title:
        url += f"&title={title}" # Type in the job title you want to search for
    return url

# Extract job data from a single job posting

def extract_job_data(job):
    # Extract job title, company name, location and salary (if available)
    title = job.find('h3', class_='base-search-card__title').get_text().strip() if job.find('h3', class_='base-search-card__title') is not None else 'N/A'
    company = job.find('h4', class_='base-search-card__subtitle').get_text().strip() if job.find('h4', class_='base-search-card__subtitle') is not None else 'N/A'
    location = job.find('span', class_='job-search-card__location').get_text().strip() if job.find('span', class_='job-search-card__location') is not None else 'N/A'
    
    # Extract salary information
    salary = job.find('span', class_='job-search-card__salary-info')
    salary_low = ''
    salary_high = ''

    # Extract salary information
    if salary is not None:
        salary = salary.get_text().strip()
        list_salary = salary.split('\n')
        salary_low = list_salary[0].strip()
        salary_high = list_salary[2].strip()
        salary = salary_low + ' - ' + salary_high
    else:
        salary = 'N/A'
    # Extract benefits information, posting date and list date
    benefits = job.find('span', class_="result-benefits__text").get_text().strip() if job.find('span', class_="result-benefits__text") is not None else 'N/A'
    posting_date = job.find('time').get_text().strip() if job.find('time') is not None else 'N/A'
    list_date = job.find('time', class_='job-search-card__listdate').get('datetime') if job.find('time', class_='job-search-card__listdate') is not None else 'N/A'


    # Additional information about the job postings
    return [title, company, location, salary, benefits, posting_date, list_date]

# Scrape job data from all pages
try:
    # Iterate through each job post
    with open(FILENAME, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(['Title', 'Company', 'Location', 'Salary', 'Benefits', 'Posted', 'List Date']) # Column headers

        # Change the range to the number of pages you want to scrape
        for i in range(50):
            # Load the next page
            url = generate_url(i) # Change the location, salary and title as per your requirements
            page = requests.get(url) # Send a HTTP request to the URL
            soup = BeautifulSoup(page.content, 'html.parser') # Parse the HTTP response into a BeautifulSoup() object
            
            # Find all job postings on the current page
            results = soup.find_all('div', class_='base-search-card__info')
            jobs = results[0:50]
            
            # Write job data to CSV
            job_data = [extract_job_data(job) for job in jobs if job is not None]
            writer.writerows(job_data)

    # Print a success message
    print('Data scraped successfully.')

# Handle exceptions
except Exception as e:
    print(f'An error occurred: {str(e)}')
